# VoxRAG

**VoxRAG** is a modern and modular RAG (Retrieval-Augmented Generation) platform designed to enable multi-document conversational AI. Users can upload files, extract and index content, and interact via a chatbot capable of answering questions based solely on uploaded knowledge.

---

## ğŸš€ Features

- ğŸ“ Upload documents (PDF, DOCX, TXT)
- ğŸ§© Automatic chunking and embedding with Sentence Transformers
- ğŸ§  Query with LangChain + Ollama (local LLMs like Mistral)
- ğŸ§  Vector search with ChromaDB
- ğŸ“¦ Containerized with Docker and Docker Compose
- ğŸŒ FastAPI backend with modular routes

---

## ğŸ§  Technologies Used

- **LangChain**: for building the RAG pipeline and question-answering chain
- **Ollama**: to run local large language models like `mistral:instruct`
- **ChromaDB**: local vector store for semantic search
- **HuggingFace Sentence Transformers**: for embedding document chunks (`all-MiniLM-L6-v2`)
- **FastAPI**: for building the backend API endpoints
- **Docker + Docker Compose**: for full development and deployment environments

---

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/your-username/voxrag.git
cd voxrag

# Build and run with Docker Compose
docker compose up --build
```

The backend will be available at `http://localhost:8000`, and the Ollama API at `http://localhost:11434`.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ rag_utils/           # Chunking, embeddings, vectorstore logic
â”‚   â”œâ”€â”€ routes/              # FastAPI endpoints (upload, extract, query...)
â”‚   â””â”€â”€ ...                  # Other internal logic
â”œâ”€â”€ start.sh                 # Entrypoint to run the FastAPI app
â”œâ”€â”€ main.py                  # FastAPI application entrypoint
â”œâ”€â”€ Dockerfile               # Backend Docker setup
â”œâ”€â”€ docker-compose.yml       # All services: backend, chroma, ollama
â””â”€â”€ requirements.txt         # Python dependencies
```

---

## ğŸ’¬ Example Usage

1. Upload a document via `/upload`
2. Extract chunks via `/extract-chunks`
3. Index content into ChromaDB via `/add-to-vectorstore`
4. Ask questions via `/ask`

---
